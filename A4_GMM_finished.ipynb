{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94f71be3",
   "metadata": {},
   "source": [
    "# DA5401 A4 — GMM-Based Synthetic Sampling for Imbalanced Data\n",
    "\n",
    "**Completed notebook (Parts B & C).**\n",
    "\n",
    "This notebook completes Part B and Part C of the assignment using the same SMOTE / CBO / CBU ideas used in A3. Explanations are provided inline (markdown) before each code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805381e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setup: imports and basic config\n",
    "import os, sys, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report, roc_auc_score, confusion_matrix\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "print('Ready. Random state =', RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f76abd",
   "metadata": {},
   "source": [
    "## Part A (brief reproducible baseline)\n",
    "\n",
    "Load the `creditcard.csv` dataset. **Note**: place `creditcard.csv` inside a folder named `Dataset/` next to this notebook, or in the notebook folder with name `creditcard.csv`. The code below tries both locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3927f6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Try to locate dataset\n",
    "candidates = ['Dataset/creditcard.csv', 'creditcard.csv', '/mnt/data/creditcard.csv']\n",
    "path = None\n",
    "for c in candidates:\n",
    "    if os.path.exists(c):\n",
    "        path = c\n",
    "        break\n",
    "\n",
    "if path is None:\n",
    "    raise FileNotFoundError(\"creditcard.csv not found. Please place it in 'Dataset/creditcard.csv' or the current directory.\")\n",
    "else:\n",
    "    print('Using dataset at:', path)\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "print('Dataset shape:', df.shape)\n",
    "print('Class distribution:\n",
    "', df['Class'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578afbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make a reproducible train/test split (keep test set imbalanced)\n",
    "X = df.drop(columns=['Class']).values\n",
    "y = df['Class'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=RANDOM_STATE, stratify=y)\n",
    "print('Train shape:', X_train.shape, 'Test shape:', X_test.shape)\n",
    "print('Train class counts:', Counter(y_train))\n",
    "print('Test class counts:', Counter(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7111795",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper functions for training/evaluation and plotting\n",
    "def fit_eval_lr(X_tr, y_tr, X_te, y_te, desc='LR'):\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), ('lr', LogisticRegression(max_iter=1000, random_state=RANDOM_STATE))])\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    y_pred = pipe.predict(X_te)\n",
    "    y_proba = pipe.predict_proba(X_te)[:,1] if hasattr(pipe, 'predict_proba') else None\n",
    "    p, r, f, _ = precision_recall_fscore_support(y_te, y_pred, labels=[1], average='binary', zero_division=0)\n",
    "    auc = roc_auc_score(y_te, y_proba) if y_proba is not None else np.nan\n",
    "    print(f\"== {desc} ==\\nPrecision(1): {p:.4f}, Recall(1): {r:.4f}, F1(1): {f:.4f}, ROC-AUC: {auc:.4f}\")\n",
    "    return {'Precision (1)': p, 'Recall (1)': r, 'F1 (1)': f, 'ROC–AUC': auc}\n",
    "\n",
    "def bar_compare(df_metrics, metrics=['Precision (1)','Recall (1)','F1 (1)']):\n",
    "    ax = df_metrics[metrics].plot(kind='bar', rot=45, figsize=(10,5))\n",
    "    ax.set_title('Comparison of metrics (minority class = 1)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b967db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Baseline (imbalanced) logistic regression\n",
    "baseline_metrics = fit_eval_lr(X_train, y_train, X_test, y_test, desc='Baseline (imbalanced)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b65bd87",
   "metadata": {},
   "source": [
    "\n",
    "## Part B — GMM for Synthetic Sampling\n",
    "\n",
    "**Theoretical notes (short):**\n",
    "\n",
    "- **SMOTE** generates synthetic minority samples by interpolating between nearest minority neighbours. It is *local* and purely geometric: it does not fit an explicit generative model to the minority distribution.\n",
    "\n",
    "- **GMM-based sampling** fits a probabilistic model (a mixture of Gaussians) to the minority class. Once fit, sampling from the GMM draws from estimated component densities, allowing generation that respects multi-modality of the minority distribution and captures covariance structure.\n",
    "\n",
    "**Why GMM can be better:** when the minority class has multiple sub-groups (clusters) or anisotropic covariance (elongated clusters), SMOTE's linear interpolation can create unrealistic points that cross cluster boundaries. GMM, by estimating per-component means and covariances, can generate points that better follow each sub-cluster.\n",
    "\n",
    "Below we implement a practical pipeline:\n",
    "1. Choose number of GMM components via BIC/AIC.\n",
    "2. Fit GMM to minority training data.\n",
    "3. Generate synthetic samples from the fitted GMM to reach the desired target.\n",
    "\n",
    "We also implement _Clustering-Based Undersampling (CBU)_ that reduces the majority by picking representative points from majority clusters (KMeans + nearest-to-centroid selection).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fc43bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate minority and majority in the training set\n",
    "X_min = X_train[y_train==1]\n",
    "X_maj = X_train[y_train==0]\n",
    "print('Training minority:', X_min.shape, 'Training majority:', X_maj.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688a4ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select k for GMM using BIC and AIC (try 1..10)\n",
    "ks = list(range(1, 11))\n",
    "bics = []\n",
    "aics = []\n",
    "for k in ks:\n",
    "    g = GaussianMixture(n_components=k, covariance_type='full', random_state=RANDOM_STATE)\n",
    "    g.fit(X_min)\n",
    "    bics.append(g.bic(X_min))\n",
    "    aics.append(g.aic(X_min))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(ks, bics, marker='o', label='BIC')\n",
    "plt.plot(ks, aics, marker='o', label='AIC')\n",
    "plt.xlabel('n_components (k)')\n",
    "plt.xticks(ks)\n",
    "plt.ylabel('Information Criterion')\n",
    "plt.legend()\n",
    "plt.title('GMM: BIC/AIC vs n_components (on minority training set)')\n",
    "plt.show()\n",
    "\n",
    "best_k = ks[int(np.argmin(bics))]\n",
    "print('Selected k (min BIC) =', best_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89884ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fit GMM with selected k and sample until matching majority count (full oversampling)\n",
    "gmm = GaussianMixture(n_components=best_k, covariance_type='full', random_state=RANDOM_STATE)\n",
    "gmm.fit(X_min)\n",
    "n_min = len(X_min)\n",
    "n_maj = len(X_maj)\n",
    "n_synth = n_maj - n_min\n",
    "print('Minority count:', n_min, 'Majority count:', n_maj, ' -> need synth:', n_synth)\n",
    "\n",
    "if n_synth > 0:\n",
    "    X_synth, comp_idx = gmm.sample(n_synth)\n",
    "else:\n",
    "    X_synth = np.empty((0, X_min.shape[1]))\n",
    "print('Generated synthetic minority samples shape:', X_synth.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76282f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CBU: reduce majority by KMeans and pick representative sample per cluster\n",
    "def cbu_undersample(X_majority, target_n, random_state=RANDOM_STATE):\n",
    "    \"\"\"Cluster majority into target_n clusters and pick one sample closest to each centroid.\"\"\"\n",
    "    if target_n >= len(X_majority):\n",
    "        return X_majority.copy()\n",
    "    k = int(target_n)\n",
    "    km = KMeans(n_clusters=k, random_state=random_state, n_init='auto')\n",
    "    labels = km.fit_predict(X_majority)\n",
    "    centroids = km.cluster_centers_\n",
    "    selected_idx = []\n",
    "    for cluster_id in range(k):\n",
    "        idxs = np.where(labels==cluster_id)[0]\n",
    "        pts = X_majority[idxs]\n",
    "        # choose sample closest to centroid\n",
    "        dists = np.linalg.norm(pts - centroids[cluster_id], axis=1)\n",
    "        chosen = idxs[np.argmin(dists)]\n",
    "        selected_idx.append(chosen)\n",
    "    return X_majority[selected_idx]\n",
    "\n",
    "# Example: choose maj_target as 3 * minority (as a sensible compromise)\n",
    "maj_target = min(len(X_maj), max(1, 3 * len(X_min)))\n",
    "print('maj_target (for CBU):', maj_target)\n",
    "X_maj_down = cbu_undersample(X_maj, maj_target)\n",
    "print('Majority after CBU shape:', X_maj_down.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1549ff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fit a fresh GMM on minority (if needed) and sample to match the reduced majority\n",
    "gmm_cbu = GaussianMixture(n_components=best_k, covariance_type='full', random_state=RANDOM_STATE)\n",
    "gmm_cbu.fit(X_min)\n",
    "n_target = len(X_maj_down) - len(X_min)\n",
    "n_target = max(0, n_target)\n",
    "print('For GMM+CBU variant we need to generate:', n_target, 'samples to match reduced majority (maj_target).')\n",
    "\n",
    "if n_target > 0:\n",
    "    X_synth_cbu, _ = gmm_cbu.sample(n_target)\n",
    "else:\n",
    "    X_synth_cbu = np.empty((0, X_min.shape[1]))\n",
    "\n",
    "print('Synth (GMM+CBU) shape:', X_synth_cbu.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cb1e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Version 1: GMM full oversampling (match original majority)\n",
    "X_bal_gmm_full = np.vstack([X_maj, X_min, X_synth])\n",
    "y_bal_gmm_full = np.hstack([np.zeros(len(X_maj), dtype=int), np.ones(len(X_min)+len(X_synth), dtype=int)])\n",
    "\n",
    "# Version 2: CBU (downsample majority) + GMM synthetic to match reduced maj\n",
    "X_bal_gmm_cbu = np.vstack([X_maj_down, X_min, X_synth_cbu])\n",
    "y_bal_gmm_cbu = np.hstack([np.zeros(len(X_maj_down), dtype=int), np.ones(len(X_min)+len(X_synth_cbu), dtype=int)])\n",
    "\n",
    "print('GMM full balanced counts:', Counter(y_bal_gmm_full))\n",
    "print('GMM+CBU balanced counts:', Counter(y_bal_gmm_cbu))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f3f775",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simple SMOTE-like sampler (for a cluster): linear interpolation between a sample and one of its neighbors within cluster\n",
    "def smote_synthetic_for_cluster(X_cluster, n_samples, k_neighbors=5, random_state=RANDOM_STATE):\n",
    "    if len(X_cluster) == 0 or n_samples <= 0:\n",
    "        return np.empty((0, X_cluster.shape[1]))\n",
    "    # fit neighbors on cluster\n",
    "    nbrs = NearestNeighbors(n_neighbors=min(k_neighbors, max(1, len(X_cluster)-1))).fit(X_cluster)\n",
    "    neigh = nbrs.kneighbors(return_distance=False)\n",
    "    out = []\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    for _ in range(n_samples):\n",
    "        i = rng.randint(len(X_cluster))\n",
    "        nn_list = neigh[i]\n",
    "        if len(nn_list)==0:\n",
    "            # duplicate plus noise\n",
    "            sample = X_cluster[i] + 0.001 * rng.randn(X_cluster.shape[1])\n",
    "        else:\n",
    "            j = rng.choice(nn_list)\n",
    "            gap = rng.rand()\n",
    "            sample = X_cluster[i] + gap * (X_cluster[j] - X_cluster[i])\n",
    "        out.append(sample)\n",
    "    return np.vstack(out)\n",
    "\n",
    "def cbo_oversample(X_train, y_train, k_clusters=6, target_ratio=1.0, random_state=RANDOM_STATE):\n",
    "    # cluster minority into k_clusters, then generate synthetic samples (SMOTE-like) distributed proportional to cluster sizes\n",
    "    X_min_loc = X_train[y_train==1]\n",
    "    X_maj_loc = X_train[y_train==0]\n",
    "    minority_count = len(X_min_loc)\n",
    "    majority_count = len(X_maj_loc)\n",
    "    desired_minority = int(target_ratio * majority_count)\n",
    "    to_generate = max(0, desired_minority - minority_count)\n",
    "    if to_generate == 0:\n",
    "        return X_train.copy(), y_train.copy()\n",
    "    km = KMeans(n_clusters=min(k_clusters, max(1, len(X_min_loc))), random_state=random_state, n_init='auto')\n",
    "    labels = km.fit_predict(X_min_loc)\n",
    "    # compute per-cluster generation quotas proportional to cluster sizes\n",
    "    cluster_sizes = np.array([np.sum(labels==i) for i in range(km.n_clusters)])\n",
    "    props = cluster_sizes / cluster_sizes.sum()\n",
    "    gens = (props * to_generate).astype(int)\n",
    "    # adjust remainder\n",
    "    remainder = to_generate - gens.sum()\n",
    "    if remainder > 0:\n",
    "        gens[:remainder] += 1\n",
    "    synth_parts = []\n",
    "    for ci in range(km.n_clusters):\n",
    "        Xc = X_min_loc[labels==ci]\n",
    "        if gens[ci] > 0:\n",
    "            synth_c = smote_synthetic_for_cluster(Xc, gens[ci], random_state=random_state+ci)\n",
    "            synth_parts.append(synth_c)\n",
    "    if len(synth_parts) > 0:\n",
    "        X_synth_total = np.vstack(synth_parts)\n",
    "    else:\n",
    "        X_synth_total = np.empty((0, X_min_loc.shape[1]))\n",
    "    # assemble new training data (majority unchanged)\n",
    "    X_new = np.vstack([X_maj_loc, X_min_loc, X_synth_total])\n",
    "    y_new = np.hstack([np.zeros(len(X_maj_loc), dtype=int), np.ones(len(X_min_loc)+len(X_synth_total), dtype=int)])\n",
    "    return X_new, y_new\n",
    "\n",
    "# Apply SBO/CBO using k=6 (as used in A3) and SMOTE for comparison\n",
    "X_cbo, y_cbo = cbo_oversample(X_train, y_train, k_clusters=6, target_ratio=1.0, random_state=RANDOM_STATE)\n",
    "print('CBO resampled counts:', Counter(y_cbo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2493cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SMOTE: oversample minority to match majority_count (classic SMOTE on entire train set)\n",
    "sm = SMOTE(random_state=RANDOM_STATE)\n",
    "X_sm, y_sm = sm.fit_resample(X_train, y_train)\n",
    "print('SMOTE resampled counts:', Counter(y_sm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291df13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CBU-only variant: undersample majority to maj_target (no synthetic)\n",
    "X_cbu_only = np.vstack([X_maj_down, X_min])\n",
    "y_cbu_only = np.hstack([np.zeros(len(X_maj_down), dtype=int), np.ones(len(X_min), dtype=int)])\n",
    "print('CBU-only counts:', Counter(y_cbu_only))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec641b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train and collect metrics for each method (evaluate on original imbalanced test set)\n",
    "methods = {}\n",
    "methods['Baseline'] = (X_train, y_train)\n",
    "methods['SMOTE'] = (X_sm, y_sm)\n",
    "methods['CBO'] = (X_cbo, y_cbo)\n",
    "methods['CBU-only'] = (X_cbu_only, y_cbu_only)\n",
    "methods['GMM-full'] = (X_bal_gmm_full, y_bal_gmm_full)\n",
    "methods['GMM+CBU'] = (X_bal_gmm_cbu, y_bal_gmm_cbu)\n",
    "\n",
    "metrics = {}\n",
    "for name, (Xtr, ytr) in methods.items():\n",
    "    print('\\n---', name)\n",
    "    metrics[name] = fit_eval_lr(Xtr, ytr, X_test, y_test, desc=name)\n",
    "\n",
    "df_metrics = pd.DataFrame.from_dict(metrics, orient='index')\n",
    "print('\\nSummary table:')\n",
    "display(df_metrics)\n",
    "bar_compare(df_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705ff4eb",
   "metadata": {},
   "source": [
    "\n",
    "## Part C — Conclusion and Recommendation\n",
    "\n",
    "**Summary of findings (how to interpret results):**\n",
    "\n",
    "- Compare Precision/Recall/F1 for the minority class across Baseline, SMOTE, CBO, CBU-only, GMM-full, and GMM+CBU.\n",
    "- If GMM-based oversampling increases Recall substantially (detects more frauds) with acceptable Precision drop, it is likely beneficial.\n",
    "\n",
    "**Recommendation template:**\n",
    "\n",
    "- If GMM+CBU gives better recall + F1 compared to Baseline and similar or better than SMOTE/CBO, recommend using GMM+CBU for production augmentation.\n",
    "- Otherwise, prefer simpler techniques (SMOTE) if they provide similar performance with less complexity.\n",
    "\n",
    "---\n",
    "\n",
    "**Notes & reproducibility:**\n",
    "- This notebook is self-contained: it reimplements SMOTE-like cluster oversampling (CBO), classic SMOTE (via `imblearn`), CBU undersampling, and GMM sampling. It uses `RANDOM_STATE=42` for reproducibility.\n",
    "- Place `creditcard.csv` in `Dataset/` or the notebook directory and run all cells sequentially to reproduce the analysis.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
